{"paragraphs":[{"text":"%md\n# Measure Engine Lab","user":"stephenp","dateUpdated":"2017-09-12T12:55:15+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Measure Engine Lab</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1505220857504_-99286593","id":"20170912-125417_146240686","dateCreated":"2017-09-12T12:54:17+0000","dateStarted":"2017-09-12T12:55:12+0000","dateFinished":"2017-09-12T12:55:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6842"},{"text":"%md\n# Quick Tips\nParagraphs start with either '%sql' for Spark SQL or '%spark' for Spark Scala.\n\n## Spark Scala\n* There are some predefined variables:\n    * spark - This is the SparkSession object. This is the entrypoint to common operations such as getting a Hive table as an DataFrame, creating new dataframes, and executing a SQL statement from a string. [Documentation](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)\n    * sc - This is the SparkContext. Partially overlaps with the SparkSession, provides lower level functions. [Documentation](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)\n* You need at most limited Scala knowledge to do basic Spark SQL stuff with DataFrame API.  The main thing you need to know is ```val df = ...``` to assign a DataFrame to an identifier. The Spark magic is the harder part.\n* General Spark Scala class and object documentation can [be found here](https://spark.apache.org/docs/latest/api/scala/index.html#package). Some points of interest: [Dataset and DataFrame](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset) and [Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column).\n* [DataFrames](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html) is actually a Dataset of an untyped Row.\n    * Operations on DataFrames are chained - sort of like a builder pattern for queries. It may feel familiar if you've worked with something like NHibernate Query objects or LINQ.\n    * ```spark.table(\"hivetablename\")``` gets you started with a DataFrame.\n    * ```df.select(col_expr ...)```\n    * ```df.where(col_expr)```\n    * ```df.join(df2, col_expr, join_type)```\n    * ```df.groupBy(col_expr).agg(count(col) , ...)```\n    * Convert a DataFrame to a strongly typed Dataset[T] using ```df.as[CaseClassType]```.\n* [Column Expressions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Column.html)\n    * Because Scala doesn't have a way to use lambdas as expression trees like with LINQ, tricks with operator overloading are used instead. Looks like lambda but really isn't.\n    * ```$(\"col\")``` , ```df(\"col\")```, ```col(\"col\")``` are column expressions\n    * triple equals (```===```) operator like ```$\"foo\" === $\"bar\"``` is another column expr for use in join, where etc. Scala can't override double-equals so Spark uses triple equals (type-safe equality) instead.\n    * ```df(col) === df2(col)``` when doing joins and need to specify which dataframe specifically.\n* Other random points\n    * fat arrow syntax for inline lambdas, e.g., ```df.map(foo => foo.bar)```\n    * once doing inline lambdas you're dealing with plain Scala, and this is opaque to Spark.\n    * Basic Scala collection stuff - [Documentation](https://www.scala-lang.org/api/current/scala/collection/Seq.html) - ```exists```, ```contains```, ```filter```, ```find```, etc.\n    * ```case class Foo(id: Int, ...)``` Trouble with Zeppelin and imports - may need to specify fully-qualified class names for case class members.\n    * Mapping Spark types to Scala - beware number sizes (int vs. long) and datetime precision (date vs timestamp).  Scala will warn you if you're doing something you can't.\n\n## Spark SQL\n* Spark SQL is just another SQL dialect.  So if you know SQL you should be able to get started. \n* [Function reference](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$). _These functions also work in Spark Scala_.\n* Only one statement per paragraph. No semicolon at the end.\n* Some introspection commands: ```show tables```, ```show databases```, ```describe measureenginelab_dev_current.encounters_patient_billing```, ```use measureenginelab_dev_current```\n\n# Other Notes\n* Code from the presentation can be cloned from here: [https://repo.advisory.com/users/stephenp/repos/big-data-measure-engine/browse](https://repo.advisory.com/users/stephenp/repos/big-data-measure-engine/browse)\n* Database for exercises: ```measureenginelab_dev_current```\n* Tables of interest: ```encounters_patient_billing```, ```procedures_patient_billing```, and ```diagnoses_patient_billing```, and ```patient_demographics_patient_billing```.\n","user":"stephenp","dateUpdated":"2017-09-12T14:15:53+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Quick Tips</h1>\n<p>Paragraphs start with either &lsquo;%sql&rsquo; for Spark SQL or &lsquo;%spark&rsquo; for Spark Scala.</p>\n<h2>Spark Scala</h2>\n<ul>\n  <li>There are some predefined variables:\n    <ul>\n      <li>spark - This is the SparkSession object. This is the entrypoint to common operations such as getting a Hive table as an DataFrame, creating new dataframes, and executing a SQL statement from a string. <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession\">Documentation</a></li>\n      <li>sc - This is the SparkContext. Partially overlaps with the SparkSession, provides lower level functions. <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession\">Documentation</a></li>\n    </ul>\n  </li>\n  <li>You need at most limited Scala knowledge to do basic Spark SQL stuff with DataFrame API. The main thing you need to know is <code>val df = ...</code> to assign a DataFrame to an identifier. The Spark magic is the harder part.</li>\n  <li>General Spark Scala class and object documentation can <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#package\">be found here</a>. Some points of interest: <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset\">Dataset and DataFrame</a> and <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column\">Column</a>.</li>\n  <li><a href=\"https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\">DataFrames</a> is actually a Dataset of an untyped Row.\n    <ul>\n      <li>Operations on DataFrames are chained - sort of like a builder pattern for queries. It may feel familiar if you&rsquo;ve worked with something like NHibernate Query objects or LINQ.</li>\n      <li><code>spark.table(&quot;hivetablename&quot;)</code> gets you started with a DataFrame.</li>\n      <li><code>df.select(col_expr ...)</code></li>\n      <li><code>df.where(col_expr)</code></li>\n      <li><code>df.join(df2, col_expr, join_type)</code></li>\n      <li><code>df.groupBy(col_expr).agg(count(col) , ...)</code></li>\n      <li>Convert a DataFrame to a strongly typed Dataset[T] using <code>df.as[CaseClassType]</code>.</li>\n    </ul>\n  </li>\n  <li><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Column.html\">Column Expressions</a>\n    <ul>\n      <li>Because Scala doesn&rsquo;t have a way to use lambdas as expression trees like with LINQ, tricks with operator overloading are used instead. Looks like lambda but really isn&rsquo;t.</li>\n      <li><code>$(&quot;col&quot;)</code> , <code>df(&quot;col&quot;)</code>, <code>col(&quot;col&quot;)</code> are column expressions</li>\n      <li>triple equals (<code>===</code>) operator like <code>$&quot;foo&quot; === $&quot;bar&quot;</code> is another column expr for use in join, where etc. Scala can&rsquo;t override double-equals so Spark uses triple equals (type-safe equality) instead.</li>\n      <li><code>df(col) === df2(col)</code> when doing joins and need to specify which dataframe specifically.</li>\n    </ul>\n  </li>\n  <li>Other random points\n    <ul>\n      <li>fat arrow syntax for inline lambdas, e.g., <code>df.map(foo =&gt; foo.bar)</code></li>\n      <li>once doing inline lambdas you&rsquo;re dealing with plain Scala, and this is opaque to Spark.</li>\n      <li>Basic Scala collection stuff - <a href=\"https://www.scala-lang.org/api/current/scala/collection/Seq.html\">Documentation</a> - <code>exists</code>, <code>contains</code>, <code>filter</code>, <code>find</code>, etc.</li>\n      <li><code>case class Foo(id: Int, ...)</code> Trouble with Zeppelin and imports - may need to specify fully-qualified class names for case class members.</li>\n      <li>Mapping Spark types to Scala - beware number sizes (int vs. long) and datetime precision (date vs timestamp). Scala will warn you if you&rsquo;re doing something you can&rsquo;t.</li>\n    </ul>\n  </li>\n</ul>\n<h2>Spark SQL</h2>\n<ul>\n  <li>Spark SQL is just another SQL dialect. So if you know SQL you should be able to get started.</li>\n  <li><a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\">Function reference</a>. <em>These functions also work in Spark Scala</em>.</li>\n  <li>Only one statement per paragraph. No semicolon at the end.</li>\n  <li>Some introspection commands: <code>show tables</code>, <code>show databases</code>, <code>describe measureenginelab_dev_current.encounters_patient_billing</code>, <code>use measureenginelab_dev_current</code></li>\n</ul>\n<h1>Other Notes</h1>\n<ul>\n  <li>Code from the presentation can be cloned from here: <a href=\"https://repo.advisory.com/users/stephenp/repos/big-data-measure-engine/browse\">https://repo.advisory.com/users/stephenp/repos/big-data-measure-engine/browse</a></li>\n  <li>Database for exercises: <code>measureenginelab_dev_current</code></li>\n  <li>Tables of interest: <code>encounters_patient_billing</code>, <code>procedures_patient_billing</code>, and <code>diagnoses_patient_billing</code>, and <code>patient_demographics_patient_billing</code>.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1505218551197_587162929","id":"20170912-121551_2683107","dateCreated":"2017-09-12T12:15:51+0000","dateStarted":"2017-09-12T14:15:53+0000","dateFinished":"2017-09-12T14:15:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6843"},{"text":"%md\n# Exercises\nUse a separate paragraph for each exercise.\n\n1. Warm up with some Spark SQL: count the number of ICD-9 or ICD-10 procedures for each encounter. Limit to top 100 records (hint: use the ```limit 100``` clause). Should return a count of zero (not null) if there are no procedures for the encounter.\n\n2. Perform the same operation as #1, but use Spark Scala by manipulating dataframes. Start with the DataFrame for both tables (```val procedures = spark.table(\"measureenginelab_dev_current.procedures_patient_billing\")```) and then finally show the results with ```df.show()```.\n\n3. Produce a Dataset of encounter objects:\n    a. Create a case class for Diagnosis. Include code, code_system, sequence, and present_on_admission.\n    b. Create a case class for Encounter. Include encounter_id, admission_date, and the ```Seq[Diagnosis]``` from 3a.\n    c. Create a Spark Scala DataFrame/DataSet query which queries the base hive tables, shapes the data into an \"encounter docoument\", and returns a DataSet[Encounter] object.\n    d. Show top 15 records. Hint: you can show the first 15 records with ```encounters.show(15)```\n    \n4. Add a little lambda. Take the Dataset from step 3.\n    a. Produce an output case class model (\"MeasureResults\") which contains: encounter_id, admission_datetime, and a boolean value called has_liveborn_diagnosis.\n    b. Create a map function which evaluates the input incounter model and calculates the ```has_liveborn_diagnosis``` which is true if the Primary Diagnosis (```sequence === 1```) is either ICD-9-CM \"V3000\" or  ICD-10-CM \"Z3800\".\n    c. Produce a new Dataset[MeasureResults].\n    d. Show top 5 records.\n\n5. Take code from step 4. Add filtering where age at admission time is between 18 and 30 and gender is \"F\".","user":"stephenp","dateUpdated":"2017-09-12T14:04:16+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Exercises</h1>\n<p>Use a separate paragraph for each exercise.</p>\n<ol>\n  <li>\n  <p>Warm up with some Spark SQL: count the number of ICD-9 or ICD-10 procedures for each encounter. Limit to top 100 records (hint: use the <code>limit 100</code> clause). Should return a count of zero (not null) if there are no procedures for the encounter.</p></li>\n  <li>\n  <p>Perform the same operation as #1, but use Spark Scala by manipulating dataframes. Start with the DataFrame for both tables (<code>val procedures = spark.table(&quot;measureenginelab_dev_current.procedures_patient_billing&quot;)</code>) and then finally show the results with <code>df.show()</code>.</p></li>\n  <li>\n  <p>Produce a Dataset of encounter objects:<br/>a. Create a case class for Diagnosis. Include code, code_system, sequence, and present_on_admission.<br/>b. Create a case class for Encounter. Include encounter_id, admission_date, and the <code>Seq[Diagnosis]</code> from 3a.<br/>c. Create a Spark Scala DataFrame/DataSet query which queries the base hive tables, shapes the data into an &ldquo;encounter docoument&rdquo;, and returns a DataSet[Encounter] object.<br/>d. Show top 15 records. Hint: you can show the first 15 records with <code>encounters.show(15)</code></p></li>\n  <li>\n  <p>Add a little lambda. Take the Dataset from step 3.<br/>a. Produce an output case class model (&ldquo;MeasureResults&rdquo;) which contains: encounter_id, admission_datetime, and a boolean value called has_liveborn_diagnosis.<br/>b. Create a map function which evaluates the input incounter model and calculates the <code>has_liveborn_diagnosis</code> which is true if the Primary Diagnosis (<code>sequence === 1</code>) is either ICD-9-CM &ldquo;V3000&rdquo; or ICD-10-CM &ldquo;Z3800&rdquo;.<br/>c. Produce a new Dataset[MeasureResults].<br/>d. Show top 5 records.</p></li>\n  <li>\n  <p>Take code from step 4. Add filtering where age at admission time is between 18 and 30 and gender is &ldquo;F&rdquo;.</p></li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1505220939272_1736239851","id":"20170912-125539_584282028","dateCreated":"2017-09-12T12:55:39+0000","dateStarted":"2017-09-12T14:04:10+0000","dateFinished":"2017-09-12T14:04:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6844"},{"text":"%sql\nselect patient_id, first_name, last_name, dateof_birth from measureenginelab_dev_current.patient_demographics_patient_billing limit 5","user":"stephenp","dateUpdated":"2017-09-12T14:12:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql","editOnDblClick":false},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"patient_id\tfirst_name\tlast_name\tdateof_birth\n4279493\tLeonor\tHatt\t1950-01-01 00:00:00.0\n4279895\tShaquille\tDejarme\t1991-01-01 00:00:00.0\n4285025\tRupert\tSarwinski\t1993-01-01 00:00:00.0\n4285971\tAugustine\tStrothman\t1936-01-01 00:00:00.0\n4287146\tFlem\tWestberry\t1957-01-01 00:00:00.0\n"}]},"apps":[],"jobName":"paragraph_1505218643816_1953375625","id":"20170912-121723_1425214753","dateCreated":"2017-09-12T12:17:23+0000","dateStarted":"2017-09-12T14:12:40+0000","dateFinished":"2017-09-12T14:12:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6845"},{"text":"%spark\n2 + 2","user":"stephenp","dateUpdated":"2017-09-12T14:11:02+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505220392884_-906212305","id":"20170912-124632_2068137924","dateCreated":"2017-09-12T12:46:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6846","dateFinished":"2017-09-12T14:11:02+0000","dateStarted":"2017-09-12T14:11:02+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res20: Int = 4\n"}]}},{"text":"%spark\n","user":"stephenp","dateUpdated":"2017-09-12T14:11:02+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505225462761_1552196296","id":"20170912-141102_1571906323","dateCreated":"2017-09-12T14:11:02+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7245"}],"name":"Measure Engine Template","id":"2CTNH842B","angularObjects":{"2CJ81T1ZZ:shared_process":[],"2CHTQKR85:shared_process":[],"2CJY6KGYD:shared_process":[],"2CHWN5JRY:shared_process":[],"2CHHKMMEK:shared_process":[],"2CJE7P6SC:shared_process":[],"2CKR27833:shared_process":[],"2CKS8E9P6:shared_process":[],"2CHFQKGQV:shared_process":[],"2CG2Z29WW:shared_process":[],"2CJU2CWZM:shared_process":[],"2CHW25497:shared_process":[],"2CKC53USG:shared_process":[],"2CKF4R4WS:stephenp:":[],"2CH51NTS9:shared_process":[],"2CK83K5P1:shared_process":[],"2CKCYPSSN:shared_process":[],"2CHU5V9WR:shared_process":[],"2CHXH9HP7:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{},"checkpoint":{"message":"Initial"}}